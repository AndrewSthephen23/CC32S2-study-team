# Actividad Play with Kubernetes

## â˜ï¸Objetivos:

- Comprender los conceptos bÃ¡sicos de contenedores como Docker y su orquestaciÃ³n a gran escala.
- Utilizar Kubernetes para desplegar y gestionar aplicaciones en mÃºltiples nodos de un clÃºster.
- Aprender a diseÃ±ar aplicaciones que escalen horizontalmente y manejen fallos de manera eficiente.
- Familiarizarse con prÃ¡cticas de Infraestructura como CÃ³digo (IaC) para gestionar configuraciones de forma reproducible y automatizada.
- Integrar prÃ¡cticas de DevOps para mejorar la colaboraciÃ³n y eficiencia en el desarrollo y operaciÃ³n de software.

Play with Kubernetes Classroom es una plataforma de Docker que permite obtener experiencia prÃ¡ctica con Kubernetes de forma accesible y eficiente, directamente desde el navegador, sin necesidad de instalar software adicional. El taller en "Play with Kubernetes Classroom" guÃ­a a los usuarios a travÃ©s de mÃ³dulos que abarcan tanto los conceptos bÃ¡sicos como los avanzados de Kubernetes. Para una comprensiÃ³n completa de Kubernetes en esta actividad realizare el laboratorio de Kubernetes Hands-on Workshop. el cual se encuentra en la siguiente pagina:Â [https://training.play-with-kubernetes.com/kubernetes-workshop/](https://training.play-with-kubernetes.com/kubernetes-workshop/)

# âš ï¸Observaciones

ğŸ”´Algunos comandos del laboratorio no se ejecutaron correctamente. Para abordar esto, he explicado las razones detrÃ¡s de estos problemas utilizando las salidas de las consolas como evidencia. 

ğŸ”´Debido a que algunas salidas son extensas y no son legibles en una simple captura de pantalla, las he mostrado en formato de cÃ³digo en el archivo README.

Nos dirigimos al laboratorio Kubernets for begiineners:Â [link del laboratorio](https://training.play-with-kubernetes.com/kubernetes-workshop/)

![Untitled](Imagenes/Untitled.png)

# âœ…Introduccion

En este taller prÃ¡ctico, exploraremos los fundamentos de Kubernetes mediante la interacciÃ³n con la plataforma a travÃ©s de la lÃ­nea de comandos. TambiÃ©n desplegaremos la aplicaciÃ³n de ejemplo Dockercoins en dos nodos de trabajo: **nodo1 y nodo2**.

# âœ…Primeros Pasos

Â¿QuÃ© son estos terminales en el navegador?

En el panel derecho, tengo abiertas dos terminales. IniciÃ© sesiÃ³n utilizando el comando docker login, ingresando con mi Docker ID o una cuenta de GitHub. Ambos terminales estÃ¡n completamente operativos. Cuando encuentro bloques de cÃ³digo, puedo interactuar con ellos haciendo clic para autocompletarlos o copiÃ¡ndolos y pegÃ¡ndolos. Ahora ejecutÃ© el comando ls y notÃ© que existe un archivo llamado anaconda-ks.cfg.

```bash
ls
```

![Untitled](Imagenes/Untitled%201.png)

## âœ…**Paso1 : Iniciar el ClÃºster**

Inicializamos el clÃºster en el primer terminal:

```bash
kubeadm init --apiserver-advertise-address $(hostname -i)
```

![Untitled](Imagenes/Untitled%202.png)

```bash
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.0.18:6443 --token nk8zwv.hxgo3xb7odfhtpwf \
        --discovery-token-ca-cert-hash sha256:68b75f61d5e5eaee2a58e237a57c5608c49ff0903b139cd22a2cadcf30ecf22c 
Waiting for api server to startup
Warning: resource daemonsets/kube-proxy is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
daemonset.apps/kube-proxy configured
No resources 
```

ğŸ‘‰ Utilizo el comando `kubeadm init --apiserver-advertise-address $(hostname -i)` para inicializar un nodo maestro en un clÃºster de Kubernetes, configurando la direcciÃ³n IP anunciada del servidor API.

Copio toda la lÃ­nea que comienza conÂ `kubeadm join`Â del primer terminal y lo pego en el segundo terminal.

```bash
kubeadm join 192.168.0.18:6443 --token nk8zwv.hxgo3xb7odfhtpwf \
        --discovery-token-ca-cert-hash sha256:68b75f61d5e5eaee2a58e237a57c5608c49ff0903b139cd22a2cadcf30ecf22c 
```

Se nos mostrara como salida :

![Untitled](Imagenes/Untitled%203.png)

```bash
Initializing machine ID from random generator.
W0702 20:21:54.146831    4804 initconfiguration.go:120] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket" with value "/run/docker/containerd/containerd.sock". Please update your configuration!
[preflight] Running pre-flight checks
        [WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
[preflight] The system verification failed. Printing the output from the verification:
KERNEL_VERSION: 4.4.0-210-generic
OS: Linux
CGROUPS_CPU: enabled
CGROUPS_CPUACCT: enabled
CGROUPS_CPUSET: enabled
CGROUPS_DEVICES: enabled
CGROUPS_FREEZER: enabled
CGROUPS_MEMORY: enabled
CGROUPS_PIDS: enabled
CGROUPS_HUGETLB: enabled
CGROUPS_BLKIO: enabled
        [WARNING SystemVerification]: failed to parse kernel config: unable to load kernel module: "configs", output: "", err: exit status 1
        [WARNING FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables does not exist
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

[node2 ~]$ 
```

ğŸ‘‰ Durante la inicializaciÃ³n, se genera un ID de mÃ¡quina aleatorio. Se muestra una advertencia sobre el uso obsoleto de los puntos finales de CRI sin un esquema de URL, que ahora se corrige automÃ¡ticamente al aÃ±adir "unix" al valor de "criSocket". Se realizan comprobaciones previas al vuelo y se advierte sobre la activaciÃ³n del intercambio de memoria, que debe desactivarse en entornos de producciÃ³n a menos que se estÃ© probando la funciÃ³n NodeSwap del kubelet. Una vez completado el proceso, se informa que este nodo se ha unido al clÃºster y se puede verificar su inclusiÃ³n ejecutando `kubectl get nodes` en el plano de control.

Esto indica que estamos casi listos. Finalmente, vamos a inicializar la red de mi clÃºster en el primer terminal.

```bash
kubectl apply -n kube-system -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
```

Al ejecutar este comando en la terminal del **nodo 1** obtengo la siguiente salida:

![Untitled](Imagenes/Untitled%204.png)

ğŸ‘‰La salida del comando `kubectl apply` intenta configurar Weave Net en Kubernetes, especÃ­ficamente en el espacio de nombres kube-system. Sin embargo, el proceso no se completa debido a un problema de conexiÃ³n con el servidor de Kubernetes. El error "Unable to connect to the server" indica que Kubernetes no pudo resolver la direcciÃ³n cloud.weave.works, probablemente debido a un fallo en la resoluciÃ³n DNS dentro del entorno de Kubernetes en el nodo node1. No pude resolver este problema, pero aÃºn asÃ­ explicarÃ© lo que se esperarÃ­a obtener al ejecutar ese comando segÃºn lo proporcionado en el taller.

Tenemos que ver una salida como esta:

```bash
kubectl apply -n kube-system -f \
  >     "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 |tr -d '\n')"
  serviceaccount "weave-net" created
  clusterrole "weave-net" created
  clusterrolebinding "weave-net" created
  role "weave-net" created
  rolebinding "weave-net" created
  daemonset "weave-net" created
```

ğŸ‘‰ El comando `kubectl apply` se emplea para implementar la configuraciÃ³n de Weave Net en el espacio de nombres kube-system, obtenida dinÃ¡micamente desde [https://cloud.weave.works/k8s/net](https://cloud.weave.works/k8s/net). Este URL incluye el parÃ¡metro `k8s-version`, el cual se genera codificando en base64 la versiÃ³n de Kubernetes utilizando `kubectl version`. Los recursos necesarios para Weave Net, como serviceaccounts, clusterroles, clusterrolebindings, roles, rolebindings y un daemonset especÃ­fico, han sido creados satisfactoriamente.

## â“**Â¿QuÃ© es esta aplicaciÃ³n?**

La aplicaicon DockerCoins funciona de la aiguietne manera:

- el trabajador solicita al rng que genere algunos bytes aleatorios
- el trabajador alimenta estos bytes al hasher y repite el proceso indefinidamente
- cada segundo, el trabajador actualiza redis para indicar cuÃ¡ntos bucles se han completado
- la interfaz web consulta redis, calcula y muestra la "velocidad de hash" en tu navegador

ğŸ‘‰ DockerCoins utiliza un trabajador que solicita al generador de nÃºmeros aleatorios (rng) que cree bytes aleatorios. Estos bytes son procesados por un hasher en un ciclo continuo. Cada segundo, el trabajador actualiza Redis con el nÃºmero de ciclos completados. DespuÃ©s, una interfaz web consulta Redis, calcula y muestra la velocidad de hash en el navegador del usuario.

## âœ…**Obtener el cÃ³digo fuente de la aplicaciÃ³n**

El taller tiene una aplicaciÃ³n de ejemplo para partes del taller. La aplicaciÃ³n estÃ¡ en el repositorioÂ `dockercoins`.

![Untitled](Imagenes/Untitled%205.png)

ğŸ‘‰ El comando `git clone` se utiliza para copiar un repositorio Git desde la URL especificada. En este caso, se estÃ¡ clonando el repositorio dockercoins desde [https://github.com/dockersamples/dockercoins](https://github.com/dockersamples/dockercoins). Durante la ejecuciÃ³n, se muestra el progreso del clonado, que incluye la enumeraciÃ³n de objetos, el conteo y la compresiÃ³n de objetos, seguido de la recepciÃ³n exitosa de los objetos.

## âœ…**Ejecutar la aplicaciÃ³n**

Voy al directorioÂ `dockercoins`Â en el repositorio clonado:

```bash
cd ~/dockercoins
```

![Untitled](Imagenes/Untitled%206.png)

ğŸ‘‰El comando `cd ~/dockercoins` se usa para mover el directorio actual del usuario a `dockercoins`, que se encuentra en su directorio personal (`~`). La salida indica que el directorio actual ha cambiado de `~/` (directorio personal) a `~/dockercoins/`, lo que confirma que ahora estÃ¡s trabajando dentro del directorio `dockercoins` en el nodo node1.

Utilizo Compose para construir y ejecutar todos los contenedores:

```bash
docker-compose up
```

![Untitled](Imagenes/Untitled%207.png)

ğŸ‘‰Cuando intento ejecutar `docker-compose up`, surge un problema con el contenedor `worker_1`, que no puede resolver el nombre `hasher` para establecer una conexiÃ³n HTTP en el puerto 80. Este error indica dificultades en la resoluciÃ³n de nombres dentro de la red de contenedores. No he podido resolver este problema, por lo que continuarÃ© con el taller basÃ¡ndome Ãºnicamente en las instrucciones proporcionadas para la ejecuciÃ³n de esta aplicaciÃ³n.

Docker Compose deberÃ­a indicar a Docker que construya todas las imÃ¡genes de contenedor (descargando las imÃ¡genes base necesarias), luego iniciar todos los contenedores y mostrar los registros combinados.

## âœ…**GeneraciÃ³n de muchos registros**

La aplicaciÃ³n deberÃ­a estar generando registros continuamente. En estos registros, deberÃ­a ver cÃ³mo el servicio de trabajador realiza solicitudes al generador de nÃºmeros aleatorios (**rng**) y al **hasher**. Para mantener la vista principal organizada y evitar saturarla, encuentro Ãºtil ejecutar estos procesos en segundo plano.

## âœ…**ConexiÃ³n a la interfaz web**

El contenedor de la interfaz web debe ofrecerme un panel de control accesible. Para verlo, solo necesito abrir un navegador web y conectarme al nodo1 en el puerto 8000, que se configurÃ³ al ejecutar la aplicaciÃ³n.

## âœ…**Limpieza**

Antes de continuar, apaguemos todo presionando **Ctrl-C.**

![Untitled](Imagenes/Untitled%208.png)

## ğŸ“š**Conceptos de Kubernetes**

Kubernetes es un sistema de gestiÃ³n de contenedores que ejecuto y administro aplicaciones en contenedores dentro de mi clÃºster. Esto significa que Kubernetes me proporciona una plataforma sÃ³lida para escalar, gestionar y mantener aplicaciones distribuidas en entornos de producciÃ³n. Me permite automatizar el despliegue, la escalabilidad y la gestiÃ³n de recursos de mis aplicaciones, asegurando que funcionen de manera eficiente y confiable en todo momento.

## **Cosas bÃ¡sicas podemos pedirle a Kubernetes que haga**

- Podemos instruir a Kubernetes para llevar a cabo tareas esenciales como iniciar 5 contenedores utilizando la imagen `atseashop/api:v1.3` y establecer un balanceador de carga interno para gestionar el trÃ¡fico hacia estos contenedores.
- TambiÃ©n podemos iniciar 10 contenedores utilizando la imagen `atseashop/webfront.:v1.3` y configurar un balanceador de carga pÃºblico para distribuir las solicitudes entrantes.
- En dÃ­as de alto trÃ¡fico, como durante el Black Friday o la Navidad, podemos escalar nuestro clÃºster agregando mÃ¡s contenedores. AdemÃ¡s, podemos actualizar a una nueva versiÃ³n reemplazando los contenedores existentes con la imagen `atseashop/webfront:v1.4`, asegurando que el procesamiento de solicitudes continÃºe sin interrupciones al actualizar los contenedores uno a uno.

## **Otras cosas que Kubernetes puede hacer por nosotros**

- Kubernetes ofrece varias funcionalidades adicionales, como el autoscaling bÃ¡sico que ajusta automÃ¡ticamente los recursos segÃºn la demanda.
- TambiÃ©n permite implementar despliegues blue/green y canarios para lanzar nuevas versiones de aplicaciones de manera controlada y segura. Facilita la ejecuciÃ³n de servicios de larga duraciÃ³n y trabajos por lotes, asÃ­ como la optimizaciÃ³n del clÃºster mediante la sobrecomisiÃ³n y el desalojo de trabajos de baja prioridad para mejorar la eficiencia de los recursos.
- AdemÃ¡s, Kubernetes es capaz de manejar servicios con datos persistentes, como bases de datos, y proporciona un control detallado de acceso para gestionar los permisos de usuarios en los recursos.
- Facilita la integraciÃ³n con servicios de terceros a travÃ©s de un catÃ¡logo de servicios y permite la automatizaciÃ³n de tareas complejas mediante operadores especializados.

## ğŸ‘¨â€ğŸ’»**Arquitectura de Kubernetes**

La arquitectura de Kubernetes se fundamenta en un conjunto de servicios que conforman su "cerebro": el servidor de API, que actÃºa como el punto de entrada para todas las operaciones; servicios crÃ­ticos como el scheduler y el controlador; y etcd, un almacÃ©n de datos clave/valor altamente disponible que sirve como la "base de datos" de Kubernetes. Estos servicios se agrupan para formar lo que se denomina el "master". Pueden ejecutarse directamente en un host o dentro de contenedores, y existe la opciÃ³n de ejecutar etcd en mÃ¡quinas separadas o en la misma infraestructura. Para asegurar la alta disponibilidad, es posible configurar mÃ¡s de un master, aunque al menos uno es necesario para las operaciones bÃ¡sicas.

### **Arquitectura de Kubernetes: los nodos**

Los nodos que alojan nuestros contenedores estÃ¡n equipados con varios servicios adicionales: un motor de contenedores (normalmente **Docker**), **kubelet** (el agente del nodo) y **kube-proxy** (un componente de red esencial pero no completo). Anteriormente, estos nodos solÃ­an llamarse **"minions"**. Es comÃºn evitar ejecutar aplicaciones en el nodo o nodos que tambiÃ©n manejan componentes del master, excepto en entornos de desarrollo con clÃºsteres pequeÃ±os.

## ğŸ‘¨â€ğŸ’»**Recursos de Kubernetes**

La API de Kubernetes define una amplia gama de objetos conocidos como recursos, los cuales se organizan por tipo o **"Kind"** en la API. Entre los tipos de recursos mÃ¡s comunes se incluyen: **nodo** (una mÃ¡quina fÃ­sica o virtual dentro del clÃºster), **pod** (un grupo de contenedores que se ejecutan conjuntamente en un nodo), **servicio** (un punto de acceso estable para conectarse a uno o varios contenedores), **namespace** (un grupo mÃ¡s o menos aislado de recursos), **secret** (un conjunto de datos sensibles destinados a ser utilizados por un contenedor), y muchos otros mÃ¡s. La lista completa de recursos disponibles se puede consultar utilizando el comando `kubectl get`.

## âš”ï¸**Declarativo vs imperativo**

En Kubernetes, se prefiere utilizar principalmente un enfoque declarativo. En este enfoque, se describe quÃ© resultado se desea alcanzar sin especificar los pasos individuales para lograrlo. Por ejemplo, serÃ­a como decir *"Quiero una taza de tÃ©".* En contraste, el enfoque imperativo detalla los pasos especÃ­ficos necesarios para alcanzar el objetivo, como *"Hierve agua, viÃ©rtela en una tetera, aÃ±ade hojas de tÃ©, deja reposar y sirve en una taza"*. El enfoque declarativo puede parecer mÃ¡s simple siempre que se conozcan los pasos requeridos para obtener el resultado deseado.

### â“**Â¿QuÃ© serÃ­a realmente declarativo?**

En el contexto de Kubernetes, adoptar un enfoque verdaderamente declarativo implica describir el resultado deseado sin detallar los pasos individuales para lograrlo. Por ejemplo, serÃ­a como decir *"Quiero una taza de tÃ©, preparada vertiendo una infusiÃ³n de hojas de tÃ© en una taza. Para obtener la infusiÃ³n, se deja reposar el objeto en agua caliente durante unos minutos. El agua caliente se obtiene vertiÃ©ndola en un recipiente adecuado y calentÃ¡ndola en una estufa".* Este mÃ©todo permite que Kubernetes automatice los procesos necesarios para alcanzar el estado deseado, asegurando coherencia y eficiencia en el despliegue y gestiÃ³n de aplicaciones.

### ğŸResumen

En Kubernetes, los sistemas imperativos se destacan por su simplicidad y la necesidad de reiniciar desde cero si una tarea se ve interrumpida. Por otro lado, los sistemas declarativos permiten recuperarse de interrupciones o de tareas incompletas al identificar lo que falta y ejecutar Ãºnicamente las acciones necesarias para alcanzar el estado deseado. Esto implica la capacidad crucial de observar el sistema y calcular una **"diferencia"** entre el estado actual y el estado deseado, garantizando asÃ­ una gestiÃ³n eficiente y adaptable de los recursos de la aplicaciÃ³n en Kubernetes.

### âš”ï¸**Declarativo vs imperativo en Kubernetes**

En Kubernetes, la mayorÃ­a de las configuraciones se definen a partir de una especificaciÃ³n. Al trabajar con archivos **YAML**, nos enfocamos en los campos de especificaciÃ³n que describen cÃ³mo queremos que sea el recurso. Kubernetes se encarga automÃ¡ticamente de ajustar el estado actual del recurso conforme a la especificaciÃ³n mediante diversos controladores. Cuando necesitamos modificar algÃºn recurso, simplemente actualizamos la especificaciÃ³n y Kubernetes se encarga de adaptar ese recurso segÃºn los nuevos parÃ¡metros definidos. Este enfoque posibilita una administraciÃ³n coherente y automatizada de los recursos en Kubernetes, simplificando la gestiÃ³n y escalabilidad de las aplicaciones en entornos de producciÃ³n.

## ğŸ’»**Modelo de red de Kubernetes**

En Kubernetes, nuestro clÃºster, formado por nodos y pods, funciona como una red IP plana unificada. Esto significa que todos los nodos deben poder comunicarse entre sÃ­ sin necesidad de NAT (traducciÃ³n de direcciones de red), al igual que todos los pods entre sÃ­ y los pods con los nodos. Cada pod tiene conocimiento de su propia direcciÃ³n IP sin necesidad de NAT. Kubernetes no prescribe una implementaciÃ³n especÃ­fica para la red, lo que ofrece flexibilidad para configurar y optimizar la red segÃºn las necesidades del entorno y las aplicaciones desplegadas.

### **Modelo de red de Kubernetes: lo bueno**

En el modelo de red de Kubernetes, todos los componentes pueden comunicarse directamente entre sÃ­ sin necesidad de traducciÃ³n de direcciones ni de puertos. Esto implica que los pods y los nodos pueden interactuar sin restricciones adicionales de protocolos. Sin embargo, es importante tener en cuenta que los pods no pueden conservar su direcciÃ³n IP al moverse de un nodo a otro, y estas direcciones IP no necesariamente tienen que ser "portÃ¡tiles" entre nodos. La especificaciÃ³n del modelo de red es lo suficientemente flexible como para admitir una variedad de implementaciones, lo que facilita la configuraciÃ³n y optimizaciÃ³n de la red segÃºn los requisitos especÃ­ficos del entorno y las aplicaciones desplegadas en Kubernetes.

### **Modelo de red de Kubernetes: lo menos bueno**

En el modelo de red de Kubernetes, aunque todos los componentes pueden comunicarse directamente entre sÃ­, se necesitan polÃ­ticas de red adicionales para asegurar la seguridad. Esto implica que la implementaciÃ³n de red utilizada debe ser compatible con estas polÃ­ticas. Existen varias implementaciones disponibles (Kubernetes menciona 15 en su documentaciÃ³n), lo que puede hacer difÃ­cil elegir la mÃ¡s adecuada. Aunque el modelo parece ofrecer una red de nivel 3, en realidad proporciona caracterÃ­sticas equivalentes a nivel 4, ya que la especificaciÃ³n requiere soporte para UDP y TCP pero no para rangos de puertos o paquetes IP arbitrarios. AdemÃ¡s, kube-proxy, al estar en la trayectoria de los datos al conectarse a pods o contenedores, puede afectar el rendimiento debido a su dependencia de tÃ©cnicas como proxying en userland o iptables.

### **Modelo de red de Kubernetes: en prÃ¡ctica**

En la prÃ¡ctica, hemos configurado nuestros nodos para utilizar Weave como soluciÃ³n de red sin un respaldo especÃ­fico hacia esta elecciÃ³n, simplemente porque hemos encontrado que funciona bien para nuestras necesidades particulares. No hay necesidad de preocuparse por la advertencia sobre el rendimiento de kube-proxy, a menos que estemos constantemente saturando interfaces de red de 10G, manejando millones de paquetes por segundo, operando plataformas de VOIP o juegos de alto trÃ¡fico, o realizando operaciones especializadas que involucren millones de conexiones simultÃ¡neas. En esos casos, es probable que ya estemos familiarizados con la configuraciÃ³n avanzada del kernel para optimizar el rendimiento de la red en Kubernetes.

## **Primer contacto con kubectl**

Kubectl es la herramienta principal que utilizaremos para interactuar con Kubernetes. Es una potente utilidad de lÃ­nea de comandos que simplifica el acceso y la administraciÃ³n a travÃ©s de la API de Kubernetes. Todo lo que se puede realizar con kubectl tambiÃ©n puede ejecutarse directamente mediante la API. AdemÃ¡s, puedo configurar kubectl utilizando el flag `--kubeconfig` para especificar un archivo de configuraciÃ³n, o directamente usando flags como `--server`, `--user`, entre otros. La pronunciaciÃ³n de kubectl puede variar y comÃºnmente se la puede escuchar como **"Cube C T L"**, **"Cube cuttle"** o **"Cube cuddle"**, segÃºn la preferencia del usuario.

### **kubectl get**

UtilizoÂ `kubectl get`Â para explorar los recursos de nodo en el clÃºster Kubernetes.

![Untitled](Imagenes/Untitled%209.png)

ğŸ‘‰ Cuando ejecuto `kubectl get node`, veo una lista de nodos en el clÃºster Kubernetes. En este caso, los nodos node1 y node2 estÃ¡n etiquetados como "NotReady", lo que indica que no estÃ¡n completamente disponibles para ejecutar cargas de trabajo. Ambos nodos desempeÃ±an el rol de "control-plane" y estÃ¡n ejecutando la versiÃ³n v1.27.2 de Kubernetes.

Puedo obtener esta informaciÃ³n de varias maneras equivalentes:

- `kubectl get no`
- `kubectl get nodes`

![Untitled](Imagenes/Untitled%2010.png)

![Untitled](Imagenes/Untitled%2011.png)

### **ObtenciÃ³n de salida en formato legible por mÃ¡quina**

`kubectl get`Â me permite obtener resultados en formatos como JSON, YAML o formateados directamente:

Para informaciÃ³n detallada de los nodos en formato ancho YAML:

```bash
kubectl get nodes -o wide
```

![Untitled](Imagenes/Untitled%2012.png)

ğŸ‘‰El comando proporciona una descripciÃ³n completa de los nodos en el clÃºster, mostrando su estado actual, roles asignados, tiempo transcurrido desde su inicio, versiÃ³n de Kubernetes, direcciones IP internas y externas, imagen del sistema operativo, versiÃ³n del kernel y el entorno de ejecuciÃ³n de contenedores. Se observa que tanto el nodo `node1` como el `node2` estÃ¡n marcados como NotReady, con direcciones IP internas de `192.168.0.22` y `192.168.0.23` respectivamente, ejecutando CentOS Linux 7 y utilizando containerd como su runtime de contenedores.

Reviso la informacion de los nodos en el cluster en formato yaml:

![Untitled](Imagenes/Untitled%2013.png)

```bash
[node1 ~]$ kubectl get no -o yaml
apiVersion: v1
items:
- apiVersion: v1
  kind: Node
  metadata:
    annotations:
      kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/docker/containerd/containerd.sock
      node.alpha.kubernetes.io/ttl: "0"
      volumes.kubernetes.io/controller-managed-attach-detach: "true"
    creationTimestamp: "2024-07-03T02:32:50Z"
    labels:
      beta.kubernetes.io/arch: amd64
      beta.kubernetes.io/os: linux
      kubernetes.io/arch: amd64
      kubernetes.io/hostname: node1
      kubernetes.io/os: linux
      node-role.kubernetes.io/control-plane: ""
      node.kubernetes.io/exclude-from-external-load-balancers: ""
    name: node1
    resourceVersion: "800"
    uid: 3d2317d1-095a-475a-90e5-339fa0ecf42d
  spec:
    taints:
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
    - effect: NoSchedule
      key: node.kubernetes.io/not-ready
  status:
    addresses:
    - address: 192.168.0.22
      type: InternalIP
    - address: node1
      type: Hostname
    allocatable:
      cpu: "8"
      ephemeral-storage: 65504Mi
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 32946972Ki
      pods: "110"
    capacity:
      cpu: "8"
      ephemeral-storage: 65504Mi
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 32946972Ki
      pods: "110"
    conditions:
    - lastHeartbeatTime: "2024-07-03T02:37:46Z"
      lastTransitionTime: "2024-07-03T02:32:49Z"
      message: kubelet has sufficient memory available
      reason: KubeletHasSufficientMemory
      status: "False"
      type: MemoryPressure
    - lastHeartbeatTime: "2024-07-03T02:37:46Z"
      lastTransitionTime: "2024-07-03T02:32:49Z"
      message: kubelet has no disk pressure
      reason: KubeletHasNoDiskPressure
      status: "False"
      type: DiskPressure
    - lastHeartbeatTime: "2024-07-03T02:37:46Z"
      lastTransitionTime: "2024-07-03T02:32:49Z"
      message: kubelet has sufficient PID available
      reason: KubeletHasSufficientPID
      status: "False"
      type: PIDPressure
    - lastHeartbeatTime: "2024-07-03T02:37:46Z"
      lastTransitionTime: "2024-07-03T02:32:49Z"
      message: 'container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady
        message:Network plugin returns error: cni plugin not initialized'
      reason: KubeletNotReady
      status: "False"
      type: Ready
    daemonEndpoints:
      kubeletEndpoint:
        Port: 10250
    images:
    - names:
      - registry.k8s.io/etcd@sha256:51eae8381dcb1078289fa7b4f3df2630cdc18d09fb56f8e56b41c40e191d6c83
      - registry.k8s.io/etcd:3.5.7-0
      sizeBytes: 101639218
    - names:
      - registry.k8s.io/kube-apiserver@sha256:bbbc0eb287dbb7507948b1c05ac8f221d1a504e04572e61d4700ff18b2a3afd0
      - registry.k8s.io/kube-apiserver:v1.27.15
      sizeBytes: 34094789
    - names:
      - registry.k8s.io/kube-controller-manager@sha256:9ff408d91018df95a8505149e778bc7815b261ba8798497ae9319beb2b73304a
      - registry.k8s.io/kube-controller-manager:v1.27.15
      sizeBytes: 31536477
    - names:
      - registry.k8s.io/kube-proxy@sha256:23c54b01075318fe6991b224192faf6d65e9412b954b335efe326977deb30332
      - registry.k8s.io/kube-proxy:v1.27.15
      sizeBytes: 27495464
    - names:
      - registry.k8s.io/kube-scheduler@sha256:9a7746f46e126b23098a844b5e3df34ee44b14b666d540a1e92a21ca7bbaac99
      - registry.k8s.io/kube-scheduler:v1.27.15
      sizeBytes: 18124766
    - names:
      - registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097
      - registry.k8s.io/pause:3.9
      sizeBytes: 321520
    - names:
      - registry.k8s.io/pause@sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db
      - registry.k8s.io/pause:3.6
      sizeBytes: 301773
    nodeInfo:
      architecture: amd64
      bootID: ddf52b2b-0a45-4aa1-a34d-c9f704ef0f0f
      containerRuntimeVersion: containerd://1.6.21
      kernelVersion: 4.4.0-210-generic
      kubeProxyVersion: v1.27.2
      kubeletVersion: v1.27.2
      machineID: 535f290836d14b56adc84e2e6447d691
      operatingSystem: linux
      osImage: CentOS Linux 7 (Core)
      systemUUID: 6E717E48-761F-6640-AA8F-878704CE72ED
- apiVersion: v1
  kind: Node
  metadata:
    annotations:
      kubeadm.alpha.kubernetes.io/cri-socket: /run/docker/containerd/containerd.sock
      node.alpha.kubernetes.io/ttl: "0"
      volumes.kubernetes.io/controller-managed-attach-detach: "true"
    creationTimestamp: "2024-07-03T02:33:10Z"
    labels:
      beta.kubernetes.io/arch: amd64
      beta.kubernetes.io/os: linux
      kubernetes.io/arch: amd64
      kubernetes.io/hostname: node2
      kubernetes.io/os: linux
    name: node2
    resourceVersion: "891"
    uid: 2485971c-5e95-4318-b0ee-12e1341c727b
  spec:
    taints:
    - effect: NoSchedule
      key: node.kubernetes.io/not-ready
  status:
    addresses:
    - address: 192.168.0.23
      type: InternalIP
    - address: node2
      type: Hostname
    allocatable:
      cpu: "8"
      ephemeral-storage: 65504Mi
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 32946972Ki
      pods: "110"
    capacity:
      cpu: "8"
      ephemeral-storage: 65504Mi
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 32946972Ki
      pods: "110"
    conditions:
    - lastHeartbeatTime: "2024-07-03T02:38:47Z"
      lastTransitionTime: "2024-07-03T02:33:10Z"
      message: kubelet has sufficient memory available
      reason: KubeletHasSufficientMemory
      status: "False"
      type: MemoryPressure
    - lastHeartbeatTime: "2024-07-03T02:38:47Z"
      lastTransitionTime: "2024-07-03T02:33:10Z"
      message: kubelet has no disk pressure
      reason: KubeletHasNoDiskPressure
      status: "False"
      type: DiskPressure
    - lastHeartbeatTime: "2024-07-03T02:38:47Z"
      lastTransitionTime: "2024-07-03T02:33:10Z"
      message: kubelet has sufficient PID available
      reason: KubeletHasSufficientPID
      status: "False"
      type: PIDPressure
    - lastHeartbeatTime: "2024-07-03T02:38:47Z"
      lastTransitionTime: "2024-07-03T02:33:10Z"
      message: 'container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady
        message:Network plugin returns error: cni plugin not initialized'
      reason: KubeletNotReady
      status: "False"
      type: Ready
    daemonEndpoints:
      kubeletEndpoint:
        Port: 10250
    images:
    - names:
      - registry.k8s.io/kube-proxy@sha256:23c54b01075318fe6991b224192faf6d65e9412b954b335efe326977deb30332
      - registry.k8s.io/kube-proxy:v1.27.15
      sizeBytes: 27495464
    - names:
      - registry.k8s.io/pause@sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db
      - registry.k8s.io/pause:3.6
      sizeBytes: 301773
    nodeInfo:
      architecture: amd64
      bootID: ddf52b2b-0a45-4aa1-a34d-c9f704ef0f0f
      containerRuntimeVersion: containerd://1.6.21
      kernelVersion: 4.4.0-210-generic
      kubeProxyVersion: v1.27.2
      kubeletVersion: v1.27.2
      machineID: b3385e01068743c993f2d70f39c625a2
      operatingSystem: linux
      osImage: CentOS Linux 7 (Core)
      systemUUID: 6E717E48-761F-6640-AA8F-878704CE72ED
kind: List
metadata:
  resourceVersion: ""
[node1 ~]$ 
```

ğŸ‘‰Cuando uso el comando `kubectl get no -o yaml`, obtengo la informaciÃ³n detallada de los nodos en el clÃºster en formato YAML. Este comando despliega todos los metadatos, anotaciones y configuraciones de cada nodo. Por ejemplo, el resultado muestra la versiÃ³n de la API, el tipo de recurso (Node), y las anotaciones especÃ­ficas del nodo.

### **Uso de jq con kubectl**

Es posible generar informes personalizados utilizandoÂ `kubectl`Â en combinaciÃ³n conÂ `jq`. Por ejemplo, para mostrar la capacidad de todos los nodos como objetos JSON:

```bash
kubectl get nodes -o json | jq ".items[] | {name:.metadata.name} + .status.capacity"
```

```bash
[node1 ~]$ kubectl get nodes -o json |
>       jq ".items[] | {name:.metadata.name} + .status.capacity"
{
  "name": "node1",
  "cpu": "8",
  "ephemeral-storage": "65504Mi",
  "hugepages-1Gi": "0",
  "hugepages-2Mi": "0",
  "memory": "32946972Ki",
  "pods": "110"
}
{
  "name": "node2",
  "cpu": "8",
  "ephemeral-storage": "65504Mi",
  "hugepages-1Gi": "0",
  "hugepages-2Mi": "0",
  "memory": "32946972Ki",
  "pods": "110"
}
[node1 ~]$ 
```

ğŸ‘‰Cuando ejecuto `kubectl get nodes -o json | jq ".items[] | {name:.metadata.name} + .status.capacity"`, obtengo informaciÃ³n en formato JSON sobre los nodos en el clÃºster y la proceso con jq para extraer y mostrar el nombre del nodo junto con su capacidad. El resultado indica que los nodos node1 y node2 tienen 8 CPUs, 65504Mi de almacenamiento efÃ­mero, y asÃ­ sucesivamente.

### **ExploraciÃ³n de recursos disponibles**

`kubectl`Â ofrece facilidades de introspecciÃ³n para listar todos los tipos de recursos disponibles y ver detalles especÃ­ficos de un recurso:

- Listar todos los tipos de recursos:
    
    ```bash
    kubectl get
    ```
    
- Ver detalles de un recurso:
    
    ```bash
    kubectl describe type/name
    kubectl describe type name
    ```
    
- Ver la definiciÃ³n de un tipo de recurso:
    
    ```bash
    kubectl explain type
    ```
    

### **Servicios en Kubernetes**

Un servicio en Kubernetes proporciona un punto de acceso consistente para conectarse con otros componentes. Para ver la lista de servicios en nuestro clÃºster:

```bash
kubectl get services
```

![Untitled](Imagenes/Untitled%2014.png)

Otra forma de hacerlo

```bash
kubectl get svc
```

![Untitled](Imagenes/Untitled%2015.png)

ğŸ‘‰ Ambos comandos muestran los servicios en el clÃºster de Kubernetes. En este caso, hay un servicio llamadoÂ `kubernetes`Â de tipoÂ `ClusterIP`Â con la IPÂ `10.96.0.1`, sin una IP externa asignada, que estÃ¡ escuchando en el puerto 443/TCP y ha estado activo durante 20 minutos.

### **Servicios ClusterIP**

Los servicios ClusterIP son internos y estÃ¡n disponibles solo dentro del clÃºster, siendo Ãºtiles para la introspecciÃ³n desde los contenedores.

Intento conectar al API con ClusterIP

```bash
curl -k https://10.96.0.1
```

![Untitled](Imagenes/Untitled%2016.png)

ğŸ‘‰Al ejecutar el comando `curl -k <https://10.96.0.1`>, intento acceder al servicio de Kubernetes utilizando la IP del clÃºster. La respuesta es un mensaje de error en formato JSON que indica que el acceso estÃ¡ prohibido.

### **Listado de contenedores en ejecuciÃ³n**

Los contenedores son gestionados mediante pods, que son conjuntos de contenedores que comparten recursos. Para enumerar los pods en nuestro conjunto:

```bash
kubectl get pods
```

![Untitled](Imagenes/Untitled%2017.png)

ğŸ‘‰El resultado del comando `kubectl get pods` indica que no hay recursos disponibles, es decir, que no hay pods actualmente en el espacio de nombres predeterminado de Kubernetes.

### **Namespaces**

Los namespaces facilitan la separaciÃ³n de recursos. Para mostrar una lista de los namespaces en el clÃºster:

```bash
kubectl get namespaces
```

![Untitled](Imagenes/Untitled%2018.png)

```bash
kubectl get ns
```

![Untitled](Imagenes/Untitled%2019.png)

ğŸ‘‰Ambos comandos muestran una lista de namespaces en el clÃºster de Kubernetes. Indican que los namespaces default, kube-node-lease, kube-public y kube-system estÃ¡n activos y tienen una antigÃ¼edad de 25 minutos.

### **Acceso a namespaces**

Podemos cambiar al namespace deseado usando la opciÃ³nÂ `-n`:

```bash
kubectl -n kube-system get pods
```

![Untitled](Imagenes/Untitled%2020.png)

ğŸ‘‰El comando indica el estado actual de los pods dentro del namespace `kube-system`. Los pods `coredns-5d78c9869d-j2vmr` y `coredns-5d78c9869d-pfwcb` estÃ¡n en estado pendiente. Mientras tanto, otros pods crÃ­ticos como `etcd-node1`, `kube-apiserver-node1`, `kube-controller-manager-node1`, `kube-proxy-9dqgb`, `kube-proxy-gdsnw` y `kube-scheduler-node1` estÃ¡n en ejecuciÃ³n sin reinicios y han estado activos durante los Ãºltimos 11 minutos.

### **Componentes en los pods**

Los pods incluyen varios componentes esenciales comoÂ `etcd`,Â `kube-apiserver`,Â `kube-controller-manager`,Â `kube-scheduler`,Â `kube-dns`,Â `kube-proxy`, yÂ `weave`.

### **EjecuciÃ³n de nuestro primer pod en Kubernetes**

Para lanzar un pod inicial y confirmar los recursos creados.

```bash
kubectl run pingpong --image alpine ping 8.8.8.8
```

![Untitled](Imagenes/Untitled%2021.png)

ğŸ‘‰El comando ha generado un pod llamado `pingpong` utilizando la imagen alpine, que ejecuta el comando ping dirigido a la direcciÃ³n IP `8.8.8.8`. El pod `pingpong` se ha creado con Ã©xito.

### **DetrÃ¡s de kubectl run**

El comandoÂ `kubectl run`Â crea varios recursos como deployment (`deploy/pingpong`), replica set (`rs/pingpong-xxxx`), y pod (`po/pingpong-yyyy`).

Enumero la mayorÃ­a de los tipos de recursos:

```bash
kubectl get all
```

![Untitled](Imagenes/Untitled%2022.png)

ğŸ‘‰IntentÃ© mostrar los registros del deployment llamado `pingpong`, pero recibÃ­ un error que indicaba que el deployment no se encontraba en el clÃºster.

### **Â¿QuÃ© son estas cosas diferentes?**

Un **Deployment** en Kubernetes es una entidad de nivel superior diseÃ±ada para facilitar el escalado, las actualizaciones continuas y los retrocesos de aplicaciones. Puede delegar la gestiÃ³n de los pods a conjuntos de rÃ©plicas (Replica Sets). Por otro lado, un Replica Set es un componente de nivel inferior que asegura que un nÃºmero especÃ­fico de pods idÃ©nticos estÃ© en ejecuciÃ³n y facilita el escalado horizontal. El Replication Controller es su predecesor, pero ha quedado obsoleto en comparaciÃ³n con el Replica Set.

### **Nuestro despliegue pingpong**

Cuando uso `kubectl run`, estoy creando un Deployment llamado `pingpong`. Este Deployment genera un Replica Set (`rs/pingpong-xxxx`), el cual a su vez crea un pod (`po/pingpong-yyyy`). Estos elementos colaboran para escalar automÃ¡ticamente, asegurar alta disponibilidad y facilitar actualizaciones continuas de la aplicaciÃ³n.

### **VisualizaciÃ³n de la salida del contenedor**

Para visualizar la salida de un contenedor en Kubernetes, se utiliza el comando `kubectl logs`. Es posible especificar el nombre de un pod o utilizar un selector como deployment o replica set. Por omisiÃ³n, este comando muestra los registros del primer contenedor dentro del pod.

```bash
kubectl logs deploy/pingpong
```

![Untitled](Imagenes/Untitled%2023.png)

ğŸ‘‰IntentÃ© mostrar los registros del deployment llamado pingpong, pero recibÃ­ un error que indicaba que el deployment no se encontraba en el clÃºster.

### **TransmisiÃ³n de registros en tiempo real**

Al igual que docker logs, kubectl logs incluye opciones como -f/--follow para observar registros en tiempo real y --tail para visualizar un nÃºmero especÃ­fico de lÃ­neas desde el final. Para ver los registros mÃ¡s recientes de manera continua, podemos utilizar kubectl logs --tail 1 --follow.

```bash
kubectl logs deploy/pingpong --tail 1 --follow
```

![Untitled](Imagenes/Untitled%2024.png)

ğŸ‘‰El comando intenta mostrar los registros del deployment llamado `pingpong`, solicitando Ãºnicamente la Ãºltima lÃ­nea y activando el seguimiento en tiempo real. No obstante, devuelve un error que indica que el deployment no se encontrÃ³ en el clÃºster.

### **Escalado de nuestra aplicaciÃ³n**

Podemos ajustar la escala de los contenedores (o pods) utilizando `kubectl scale`. Por ejemplo, ejecutar `kubectl scale deploy/pingpong --replicas 8` incrementarÃ­a la cantidad de rÃ©plicas del `Deployment pingpong`.

```bash
kubectl scale deploy/pingpong --replicas 8
```

![Untitled](Imagenes/Untitled%2025.png)

ğŸ‘‰El comando intenta aumentar la escala del deployment llamado `pingpong` a 8 rÃ©plicas, pero muestra un error que indica que no se encontraron objetos vÃ¡lidos para realizar el escalado.

### **Resiliencia**

El `Deployment pingpong` monitorea su `Replica Set` para garantizar que el nÃºmero adecuado de pods estÃ© en funcionamiento. Si un pod desaparece, el sistema lo detectarÃ¡ y lo reemplazarÃ¡ automÃ¡ticamente.

```bash
kubectl get pods -w
```

![Untitled](Imagenes/Untitled%2026.png)

ğŸ‘‰El comando se usa para verificar el estado actual de los pods en tiempo real en el clÃºster de Kubernetes. En esta situaciÃ³n, muestra un pod llamado `pingpong` que estÃ¡ en estado "Pending", lo cual indica que Kubernetes aÃºn no ha podido asignar este pod a ningÃºn nodo del clÃºster.

```bash
kubectl delete pod pingpong-yyyy
```

![Untitled](Imagenes/Untitled%2027.png)

ğŸ‘‰El comando intenta eliminar un pod especÃ­fico llamado `pingpong-yyyy`. Sin embargo, el error "Error from server (NotFound): pods 'pingpong-yyyy' not found" indica que no se encontrÃ³ ningÃºn pod con ese nombre en el clÃºster de Kubernetes.

### **Â¿QuÃ© pasa si queremos algo diferente?**

Si necesito ejecutar un contenedor de "uso Ãºnico" que no se reinicie, puedo utilizar `kubectl run --restart=OnFailure` o `kubectl run --restart=Never`. Estos comandos crearÃ­an trabajos (Jobs) o pods en lugar de despliegues.

### **VisualizaciÃ³n de registros de mÃºltiples pods**

Cuando se menciona el nombre de unÂ `Deployment`,Â `kubectl logs`Â visualiza los registros de un solo pod especÃ­fico. Para acceder a los registros de varios pods, es necesario emplear un selector que utilice etiquetas. Por ejemplo, ejecutarÂ `kubectl logs -l run=pingpong --tail 1`Â permitirÃ­a ver la Ãºltima lÃ­nea de registros de todos los pods etiquetados conÂ `run=pingpong`.

```bash
kubectl logs -l run=pingpong --tail 1
```

![Untitled](Imagenes/Untitled%2028.png)

ğŸ‘‰DespuÃ©s de ejecutar el comando `kubectl logs -l run=pingpong --tail 1` y no obtener ningÃºn resultado, esto podrÃ­a indicar que no hay pods actualmente en ejecuciÃ³n que tengan la etiqueta especÃ­fica `run=pingpong`.

### **Limpieza**

Para limpiar un despliegue, podemos eliminar conÂ `kubectl delete deploy/pingpong`.

```bash
kubectl delete deploy/pingpong
```

![Untitled](Imagenes/Untitled%2029.png)

ğŸ‘‰Al intentar eliminar el deployment denominado "pingpong", se genera un error que indica que no se encontrÃ³ el deployment con ese nombre. Esto podrÃ­a deberse a que el deployment "pingpong" no existe en el namespace actual o que se ha especificado incorrectamente su nombre.

### **Exponiendo contenedores**

El comando `kubectl expose` crea un servicio para los pods existentes en Kubernetes, lo cual ofrece una direcciÃ³n estable para conectarse a los pods. Se pueden utilizar diversos tipos de servicios como `ClusterIP`, `NodePort`, `LoadBalancer`, o `ExternalName` segÃºn los requisitos especÃ­ficos de conectividad y acceso necesarios.

### **EjecuciÃ³n de contenedores con puertos abiertos**

Para desplegar contenedores que requieren puertos abiertos en Kubernetes, como los de ElasticSearch, utilizo el comando `kubectl run` con la imagen especÃ­fica y especifico el nÃºmero de rÃ©plicas deseado. Puedo monitorear el estado de estos pods utilizando `kubectl get pods -w.`

### **Exponiendo nuestro despliegue**

Para habilitar el acceso a un despliegue, puedo crear un servicio ClusterIP por defecto utilizando `kubectl expose`. Por ejemplo, puedo exponer el puerto 9200 de la API HTTP de ElasticSearch utilizando el comando `kubectl expose deploy/elastic --port 9200`.

```bash
kubectl expose deploy/elastic --port 9200
```

![Untitled](Imagenes/Untitled%2030.png)

ğŸ‘‰El comando intenta exponer el servicio asociado al deployment denominado "elastic", pero devuelve un error que indica que no se encontrÃ³ el deployment con ese nombre.

DirecciÃ³n IP que se debio asignar:

![Untitled](Imagenes/Untitled%2031.png)

ğŸ‘‰El comando lista los servicios desplegados en el clÃºster actual de Kubernetes. En este caso, solo se muestra el servicio "**kubernetes**", el cual es utilizado internamente por Kubernetes para la comunicaciÃ³n entre sus componentes.

### **Los servicios son constructos de capa 4**

Puedo configurar direcciones IP para los servicios, pero estos siguen estando definidos por capa 4, lo que implica que un servicio no se reduce Ãºnicamente a una direcciÃ³n IP; tambiÃ©n incluye un protocolo especÃ­fico y un puerto. Esta limitaciÃ³n se debe a cÃ³mo estÃ¡ implementado kube-proxy actualmente, utilizando mecanismos que no permiten operar a nivel de capa 3. Por lo tanto, necesito especificar el nÃºmero de puerto para mi servicio.

### **Probando nuestro servicio**

Envio algunas solicitudes HTTP a los pods de ElasticSearch:

```bash
IP=$(kubectl get svc elastic -o go-template --template '{{ .spec.clusterIP }}')
```

![Untitled](Imagenes/Untitled%2032.png)

ğŸ‘‰El comando tratÃ³ de obtener detalles del servicio llamado "elastic" en el clÃºster Kubernetes, pero recibiÃ³ un error que indica que el servicio no pudo ser encontrado. Esto sugiere que actualmente no hay un servicio llamado "elastic" desplegado en el namespace o contexto de Kubernetes desde donde se ejecutÃ³ el comando.

Trato de enviar algunas solicitudes:

```bash
curl http://$IP:9200/
```

![Untitled](Imagenes/Untitled%2033.png)

ğŸ‘‰El comando trata de realizar una solicitud HTTP a la direcciÃ³n IP especificada en el entorno de Kubernetes, pero devuelve un error que indica que no se pudo encontrar el servidor host.

### **Limpieza**

Termino con el despliegue de ElasticSearch, asÃ­ que voy a limpiarlo:

```bash
kubectl delete deploy/elastic
```

![Untitled](Imagenes/Untitled%2034.png)

ğŸ‘‰El comando intentÃ³ eliminar el despliegue llamado "elastic", pero recibiÃ³ un error que indica que no se encontrÃ³ ningÃºn despliegue con ese nombre.

## **Nuestra aplicaciÃ³n en Kubernetes**

**Â¿QuÃ© se tiene planeado?**

En esta parte, vamos a:

- Construir imÃ¡genes para la aplicaciÃ³n,
- Publicar estas imÃ¡genes en un registro,
- Ejecutar despliegues utilizando estas imÃ¡genes,
- Exponer estos despliegues para que puedan comunicarse entre sÃ­,
- Exponer la interfaz web para acceder desde fuera.

**El plan**

1. Construir en nuestro nodo de control (node1),
2. Etiquetar las imÃ¡genes para que se nombren como $USERNAME/servicename,
3. Subirlas a Docker Hub,
4. Crear despliegues utilizando las imÃ¡genes,
5. Exponer (con un ClusterIP) los servicios que necesitan comunicarse,
6. Exponer (con un NodePort) la interfaz web.

### **ConfiguraciÃ³n**

En mi primer terminal, voy a configurar una variable de entorno para mi nombre de usuario de Docker Hub. Antes, debo asegurarme de estar ubicado en el directorio dockercoins.

```bash
export USERNAME=TU_USUARIO
pwd
```

![Untitled](Imagenes/Untitled%2035.png)

ğŸ‘‰El comando establece una variable de entorno llamada USERNAME con el valor "andrewrubel". Esta variable estarÃ¡ disponible para cualquier comando o script que se ejecute en esa sesiÃ³n de terminal, permitiendo que el valor "andrewrubel" sea utilizado como referencia en diversas operaciones dentro del entorno de trabajo actual.

![Untitled](Imagenes/Untitled%2036.png)

ğŸ‘‰El comando pwd me muestra la ruta completa del directorio actual en el que me encuentro actualmente. En este caso, el resultado indica que el directorio actual es /root, que es el directorio raÃ­z del usuario root.

### **Una nota sobre los registros**

Para este taller, usare Docker Hub. TambiÃ©n hay otras opciones, como Docker Trusted Registry y Docker Open Source Registry.

### **Docker Hub**

Docker Hub es el registro predeterminado para Docker. Los nombres de imagen en Docker Hub siguen el formato `$USERNAME/$IMAGENAME` o `$ORGANIZATIONNAME/$IMAGENAME`.

Para utilizar Docker Hub, es necesario tener una cuenta. DespuÃ©s, se debe ejecutar el comando `docker login` en el terminal e iniciar sesiÃ³n con el nombre de usuario y la contraseÃ±a de la cuenta de Docker Hub.

![Untitled](Imagenes/Untitled%2037.png)

ğŸ‘‰Cuando ejecutÃ© el comando docker login, iniciÃ© sesiÃ³n con mi Docker ID para poder descargar y subir imÃ¡genes desde Docker Hub. ProporcionÃ© mi nombre de usuario y contraseÃ±a cuando se me solicitÃ³, y Docker validÃ³ estas credenciales correctamente.

### **ConstrucciÃ³n y subida de nuestras imÃ¡genes**

Utilizare una funciÃ³n conveniente de Docker Compose. Voy al directorioÂ `stacks`, construyo y subo las imÃ¡genes:

```bash
cd ~/dockercoins/stacks
docker-compose -f dockercoins.yml build
docker-compose -f dockercoins.yml push
```

![Untitled](Imagenes/Untitled%2038.png)

ğŸ‘‰Cuando se ejecutan estos comandos en el directorio actual (./), no se encuentra y muestra un error del tipo FileNotFoundError. Esto indica que el archivo YAML necesario para la configuraciÃ³n de Docker Compose no estÃ¡ presente en el directorio desde donde se estÃ¡ ejecutando el comando.

### **Despliegue de todas las cosas**

Ahora voy a tratar de desplegar el cÃ³digo (asÃ­ como una instancia de redis). Despleguo redis y todo lo demÃ¡s:

```bash
kubectl run redis --image=redis

for SERVICE in hasher rng webui worker; do
  kubectl run $SERVICE --image=$USERNAME/$SERVICE -l app=$SERVICE
done
```

![Untitled](Imagenes/Untitled%2039.png)

ğŸ‘‰Al ejecutar el comando, se despliega un nuevo pod en Kubernetes utilizando la imagen de Redis desde el registro de contenedores por defecto.

![Untitled](Imagenes/Untitled%2040.png)

ğŸ‘‰Al ejecutar estos comandos, se estÃ¡n creando varios pods en Kubernetes de manera automatizada.

### **Â¿EstÃ¡ funcionando?**

DespuÃ©s de esperar a que se complete el despliegue, reviso los registros. Utilizo `kubectl get deploy -w` para observar los eventos de despliegue y `kubectl logs` para revisar los registros de los pods "rng" y "worker".

![Untitled](Imagenes/Untitled%2041.png)

ğŸ‘‰Cuando intento obtener los registros de los despliegues "rng" y "worker" utilizando el comando `kubectl logs`, se genera un error que indica que los despliegues especificados no fueron encontrados.

### **Internamente**

Expongo cada despliegue, especificando el puerto correcto:

```bash
kubectl expose deployment redis --port 6379
kubectl expose deployment rng --port 80
kubectl expose deployment hasher --port 80
```

![Untitled](Imagenes/Untitled%2042.png)

ğŸ‘‰Cuando intentÃ© exponer los despliegues de redis, rng y hasher utilizando el comando `kubectl expose deployment`, recibÃ­ un error que indicaba que los despliegues especificados no fueron encontrados.

### **Â¿EstÃ¡ funcionando ya?**

El worker tiene un bucle infinito que se reintentarÃ¡ cada 10 segundos despuÃ©s de un error. Transmite los registros del worker para verlo funcionar:

```bash
kubectl logs deploy/worker --follow
```

![Untitled](Imagenes/Untitled%2043.png)

ğŸ‘‰Al intentar ver los registros del despliegue llamado "worker" utilizando `kubectl logs deploy/worker --follow`, recibo un error que indica que no se encontrÃ³ ese despliegue con ese nombre en el clÃºster de Kubernetes.

### **Para acceso externo**

Ahora quiero acceder a la interfaz web. La expondre con un NodePort. Creo un servicio NodePort para la interfaz web:

```bash
kubectl create service nodeport webui --tcp=80 --node-port=30001
```

![Untitled](Imagenes/Untitled%2044.png)

ğŸ‘‰Cuando ejecuto el comando, estoy creando un nuevo servicio en Kubernetes llamado "webui" de tipo NodePort. Este servicio expone el puerto 80 de los pods seleccionados mediante el selector por defecto, permitiendo acceder a ellos desde fuera del clÃºster a travÃ©s del puerto 30001 en cada nodo del clÃºster.

Verificar que el puerto que fue asignado:

![Untitled](Imagenes/Untitled%2045.png)

### **Acceso a la interfaz web**

Ahora se deberia conectar cualquier nodo al puerto asignado para ver la interfaz web.

### **Implicaciones de seguridad de kubectl apply**

Cuando utilizo `kubectl apply -f <URL>`, estoy creando recursos que pueden ser maliciosos. Por ejemplo, un despliegue podrÃ­a:

- Iniciar mineros de Bitcoin en todo el clÃºster.
- Esconderse en un espacio de nombres no predeterminado.
- Montar enlaces al sistema de archivos de nuestros nodos.
- Insertar claves SSH en la cuenta raÃ­z del nodo.
- Encriptar nuestros datos y hacerlos inaccesibles.

kubectl apply es el equivalente moderno de "curl | sh". Usar "curl | sh" es conveniente y seguro cuando se utilizan URLs HTTPS de fuentes confiables. Del mismo modo, `kubectl apply -f` tambiÃ©n es conveniente y seguro bajo estas condiciones, pero introduce nuevos posibles puntos de fallo.

### **Escalando un despliegue**

Voy a empezar con algo sencillo: el despliegue del worker.

```bash
kubectl get pods
kubectl get deployments
kubectl scale deploy/worker --replicas=10
```

![Untitled](Imagenes/Untitled%2046.png)

![Untitled](Imagenes/Untitled%2047.png)

ğŸ‘‰Cuando ejecuto el comando, indica que no se encontraron recursos en el namespace por defecto (default), lo que sugiere que no hay despliegues activos en este momento en ese namespace.

ğŸ‘‰Al intentar escalar el deployment llamado "worker" a 10 rÃ©plicas utilizando el comando `kubectl scale`, se devuelve un error que indica "no objects passed to scale". Esto significa que el comando no pudo encontrar el despliegue llamado "worker" en el clÃºster Kubernetes.

DespuÃ©s de unos segundos, el grÃ¡fico en la interfaz web deberÃ­a aparecer (y alcanzar hasta 10 hashes/segundo, igual que cuando estaba ejecutando en uno solo).

### **Daemon sets**

Â¿QuÃ© sucede si deseo tener una instancia (y solo una) del pod "rng" por nodo? Si simplemente aumento la escala del despliegue "rng" a 2, no hay garantÃ­a de que se distribuyan adecuadamente. En lugar de utilizar un despliegue, optarÃ© por un daemon set.

### **Creando un daemon set**

Desafortunadamente, a partir de Kubernetes 1.9, la CLI no puede crear daemon sets. Sin embargo, cualquier tipo de recurso siempre se puede crear proporcionando una descripciÃ³n YAML:

```bash
kubectl apply -f foo.yaml
```

### **Creando el archivo YAML para nuestro daemon set**

Empiezo con el archivo YAML para el recursoÂ `rng`Â actual.

```bash
kubectl get deploy/rng -o yaml --export > rng.yml
```

![Untitled](Imagenes/Untitled%2048.png)

ğŸ‘‰El comando intenta exportar la configuraciÃ³n del despliegue llamado "rng" a un archivo YAML llamado rng.yml. Sin embargo, se muestra un error de "unknown flag: --export", lo cual indica que la versiÃ³n de kubectl utilizada no admite el flag --export en la operaciÃ³n kubectl get.

### **"Lanzar" un recurso a otro**

Â¿QuÃ© pasa si simplemente cambio el campoÂ `kind`?

```bash
vi rng.yml
```

CambioÂ `kind: Deployment`Â aÂ `kind: DaemonSet`, guardo y salgo.

Intento crear mi nuevo recurso:

```bash
kubectl apply -f rng.yml
```

![Untitled](Imagenes/Untitled%2049.png)

ğŸ‘‰El comando trata de aplicar las configuraciones especificadas en el archivo YAML rng.yml, pero muestra un error que indica la ausencia de objetos para realizar la aplicaciÃ³n.

### **Usando el --force, Luke**

Puedo tratar de decirle Kubernetes que ignore estos errores y lo intente de todos modos.

```bash
kubectl apply -f rng.yml --validate=false
```

![Untitled](Imagenes/Untitled%2050.png)

ğŸ‘‰El comando intenta aplicar la configuraciÃ³n del archivo rng.yml al clÃºster Kubernetes, desactivando la validaciÃ³n de recursos. El mensaje de error "no objects passed to apply" indica que el archivo rng.yml no contiene ningÃºn objeto vÃ¡lido de Kubernetes para aplicar.

### **Comprobando lo que hemos hecho**

Â¿Transforme el despliegue en un daemon set?

```bash
kubectl get all
```

![Untitled](Imagenes/Untitled%2051.png)

ğŸ‘‰El comando muestra el estado actual de todos los recursos desplegados en el clÃºster Kubernetes. En la salida se observa que varios pods (hasher, pingpong, redis, rng, webui, worker) estÃ¡n en estado "Pending", lo que indica que aÃºn no han sido asignados a ningÃºn nodo del clÃºster para su ejecuciÃ³n.

### **Â¿QuÃ© estÃ¡n haciendo todos estos pods?**

Verifico los registros de todos estos podsÂ `rng`. Todos estos pods tienen una etiquetaÂ `run=rng`.

```bash
kubectl logs -l run=rng --tail 1
```

![Untitled](Imagenes/Untitled%2052.png)

ğŸ‘‰IntentÃ© mostrar los registros del Ãºltimo contenedor que ejecutaba la etiqueta run=rng utilizando el comando. Sin embargo, en este caso, no se encontraron recursos en el espacio de nombres predeterminado que coincidieran con esa etiqueta. Esto sugiere que actualmente no hay ningÃºn pod desplegado que tenga esa etiqueta especÃ­fica.

### **Removiendo el primer pod del balanceador de carga**

Â¿QuÃ© sucederÃ­a si elimino ese pod conÂ `kubectl delete pod ...`?

- El replicaset lo recrearÃ­a inmediatamente.

Â¿QuÃ© pasarÃ­a si elimino la etiquetaÂ `run=rng`Â de ese pod?

- El replicaset lo recrearÃ­a inmediatamente.

Porque lo que importa para el replicaset es el nÃºmero de pods que coinciden con ese selector.

### **Profundizando en los selectores**

Voy a ver los selectores para el despliegueÂ `rng`Â y el replicaset asociado.

Muestro informaciÃ³n detallada sobre el despliegueÂ `rng`:

```bash
kubectl describe deploy rng
```

![Untitled](Imagenes/Untitled%2053.png)

ğŸ‘‰El comando intenta describir los detalles del despliegue llamado "rng", pero devuelve un error que indica que no se pudo encontrar ningÃºn despliegue con ese nombre en el clÃºster.

Muestro informaciÃ³n detallada sobre el replicasetÂ `rng-yyyy`:

```bash
kubectl describe rs rng-yyyy
```

![Untitled](Imagenes/Untitled%2054.png)

ğŸ‘‰El comando intenta describir los detalles de un conjunto de rÃ©plicas con el nombre "rng-yyyy", pero muestra un error que indica que no se pudo encontrar ningÃºn conjunto de rÃ©plicas con ese nombre en el clÃºster.

El selector del replicaset tambiÃ©n debe tener unÂ `pod-template-hash`, a diferencia de los pods en mi daemon set.

### **Actualizando un servicio a travÃ©s de etiquetas y selectores**

Â¿QuÃ© pasa si quiero quitar el despliegueÂ `rng`Â del balanceador de carga?

OpciÃ³n 1:

- Destruirlo.

OpciÃ³n 2:

- Agregar una etiqueta adicional al daemon set
- Actualizar el selector del servicio para que se refiera a esa etiqueta.

**Agregar una etiqueta adicional al daemon set**

Voy a actualizar la especificaciÃ³n del daemon set.

OpciÃ³n 1:

- Editar el archivoÂ `rng.yml`Â que usamos antes.
- Cargar la nueva definiciÃ³n conÂ `kubectl apply`.

OpciÃ³n 2:

- UsarÂ `kubectl edit`.

Actualizo el daemon set para agregarÂ `isactive: "yes"`Â a la etiqueta del selector y de la plantilla:

```bash
kubectl edit daemonset rng
```

![Untitled](Imagenes/Untitled%2055.png)

ğŸ‘‰IntentÃ© abrir y editar el daemonset llamado "rng" con el comando, pero recibÃ­ un error que dice que no se pudo encontrar ningÃºn daemonset con ese nombre en el clÃºster actual.

```bash
kubectl edit service rng
```

![Untitled](Imagenes/Untitled%2056.png)

ğŸ‘‰El comando intenta abrir y editar el servicio llamado "rng", pero el error indica que no se pudo encontrar ningÃºn servicio con ese nombre en el clÃºster actual.

### **Verificando lo que hemos hecho**

Verifico los registros de todos los podsÂ `run=rng`Â para confirmar que solo 2 de ellos estÃ¡n activos ahora:

```bash
kubectl logs -l run=rng
```

![Untitled](Imagenes/Untitled%2057.png)

ğŸ‘‰El comando intenta mostrar los registros de los pods que tienen la etiqueta run=rng, pero no se encontraron recursos que coincidan con esa etiqueta en el espacio de nombres por defecto.

Los sellos de tiempo deberÃ­an darme una pista sobre cuÃ¡ntos pods estÃ¡n recibiendo trÃ¡fico actualmente.

Miro los pods que tengo ahora:

```bash
kubectl get pods
```

![Untitled](Imagenes/Untitled%2058.png)

### **RecuperÃ¡ndonos de un despliegue fallido**

Podria empujar alguna imagen v0.3 (la lÃ³gica de reintentos del pod eventualmente lo imagenturarÃ¡ y el despliegue continuarÃ¡) o podria invocar un rollback manual.

Cancelar el despliegue y esperar a que el polvo se asiente:

```bash
kubectl rollout undo deploy worker
kubectl rollout status deploy worker
```

![Untitled](Imagenes/Untitled%2059.png)

ğŸ‘‰El comando intenta deshacer un despliegue previo del deployment denominado "worker", pero arroja un error que indica que el deployment especificado no fue encontrado.

### **Cambiar parÃ¡metros de rollout**

Nuestros objetivos son los siguientes:

1. Retroceder a la versiÃ³n v0.1.
2. Mantener una alta disponibilidad (siempre tener el nÃºmero deseado de workers disponibles).
3. Implementar los cambios rÃ¡pidamente (actualizar mÃ¡s de un pod a la vez).
4. Permitir que los workers se "calienten" un poco antes de continuar con mÃ¡s actualizaciones.

### **Aplicar cambios a travÃ©s de un parche YAML**

PodrÃ­a utilizar `kubectl edit deployment worker` para realizar los cambios directamente en el deployment. Alternativamente, podrÃ­a usar `kubectl patch` con el YAML exacto que se mostrÃ³ antes para aplicar los cambios de manera especÃ­fica.

Una vez aplicados todos mis cambios, debo esperar a que surtan efecto.

![Untitled](Imagenes/Untitled%2060.png)

ğŸ‘‰Estoy intentando aplicar los recursos que estÃ¡n definidos en el archivo rng.yml, pero me encuentro con un error que indica que no se encontraron objetos vÃ¡lidos para aplicar en el archivo especificado.

## **PrÃ³ximos pasos**

Bien, Â¿cÃ³mo empiezo a containerizar mis aplicaciones?

Lista de verificaciÃ³n sugerida para la containerizaciÃ³n:

- Crear un Dockerfile para cada servicio de la aplicaciÃ³n.
- Escribir Dockerfiles para los otros servicios que puedan ser compilados.
- Elaborar un archivo Compose que abarque toda la aplicaciÃ³n.
- Garantizar que los desarrolladores puedan ejecutar la aplicaciÃ³n en contenedores localmente.
- Establecer la automatizaciÃ³n de la construcciÃ³n de imÃ¡genes de contenedores desde el repositorio de cÃ³digo.
- Configurar un flujo de integraciÃ³n continua utilizando estas imÃ¡genes.
- Configurar un flujo de entrega continua para entornos de staging o QA utilizando estas imÃ¡genes.

### **Espacios de nombres (Namespaces)**

Los espacios de nombres me permiten ejecutar mÃºltiples pilas idÃ©nticas lado a lado.

Por ejemplo, puedo tener dos espacios de nombres, como azul y verde, cada uno con su propio servicio redis.

Cada servicio redis en estos espacios de nombres tiene su propio ClusterIP.

kube-dns crea dos entradas, mapeando estas direcciones IP de ClusterIP:

- redis.blue.svc.cluster.local
- redis.green.svc.cluster.local

Los pods en el espacio de nombres azul obtienen un sufijo de bÃºsqueda de blue.svc.cluster.local.

Por lo tanto, al resolver "redis" desde un pod en el espacio de nombres azul, se obtiene el redis que estÃ¡ "local" a ese espacio de nombres.

Sin embargo, esto no proporciona aislamiento. Esa serÃ­a la funciÃ³n de las polÃ­ticas de red.

### **Servicios persistentes (databases, etc.)**

Como primer paso, es mÃ¡s prudente mantener los servicios persistentes fuera del clÃºster.

Existen varias soluciones para exponerlos a los pods:

1. Utilizar Servicios ExternalName (por ejemplo, redis.blue.svc.cluster.local se convierte en un registro CNAME).
2. Emplear Servicios ClusterIP con Endpoints explÃ­citos en lugar de que Kubernetes genere los endpoints a partir de un selector.
3. Implementar Servicios Ambassador, que son proxies a nivel de aplicaciÃ³n capaces de proporcionar inyecciÃ³n de credenciales y otras funcionalidades adicionales.

### **Manejo del trÃ¡fico HTTP**

Los servicios son construcciones de capa 4.

HTTP es un protocolo de capa 7.

Es gestionado por ingresses (un recurso separado).

Los ingresses permiten:

- Enrutamiento virtual basado en hosts.
- Persistencia de sesiÃ³n.
- Mapeo de URI.
- Â¡Y muchas otras funcionalidades adicionales!

### **Registro y mÃ©tricas**

El registro es gestionado por el motor de contenedores.

Las mÃ©tricas suelen ser manejadas con Prometheus.

### **Manejo de la configuraciÃ³n de nuestras aplicaciones**

Dos recursos son especialmente Ãºtiles: secrets y config maps.

Estos recursos permiten proporcionar informaciÃ³n arbitraria a nuestros contenedores.

Es crucial evitar almacenar la configuraciÃ³n dentro de las imÃ¡genes de los contenedores (aunque hay excepciones a esta regla, por lo general no es recomendable).

Nunca se debe almacenar informaciÃ³n sensible en las imÃ¡genes de los contenedores, ya que esto equivale a tener contraseÃ±as escritas en notas adhesivas en la pantalla.

### **FederaciÃ³n de clÃºsteres**

La efectividad de Kubernetes se basa en el funcionamiento de etcd.

etcd opera con el protocolo Raft.

Raft sugiere mantener baja latencia entre los nodos.

**Â¿QuÃ© pasa si el clÃºster se extiende a mÃºltiples regiones?**

Debe implementarse en clÃºsteres locales.

Unirlos en una federaciÃ³n de clÃºsteres.

Sincronizar recursos entre los clÃºsteres.

Descubrir recursos entre los clÃºsteres.

### **Experiencia del desarrollador**

â“Â¿CÃ³mo integras a un nuevo desarrollador?

âœ…Cuando se incorpora a un nuevo desarrollador al equipo, es crucial que configure Docker y Kubernetes en su mÃ¡quina local para ejecutar la pila de desarrollo de manera consistente. Esto garantiza que todos los miembros del equipo trabajen en un entorno reproducible y uniforme. AdemÃ¡s, es fundamental que el nuevo desarrollador se familiarice con los Dockerfiles y los archivos Compose, ya que esto facilita la comprensiÃ³n de la estructura y la interacciÃ³n de los servicios dentro del entorno Kubernetes.

â“Â¿QuÃ© necesitan instalar para obtener una pila de desarrollo?

âœ…Para establecer una pila de desarrollo efectiva, los nuevos desarrolladores deben instalar varias herramientas esenciales. Esto incluye configurar Docker en sus mÃ¡quinas para gestionar contenedores que alojen cada servicio de la aplicaciÃ³n. AdemÃ¡s, es fundamental configurar un entorno local de Kubernetes utilizando herramientas como minikube o Docker Desktop, junto con kubectl para interactuar con el clÃºster Kubernetes local. Esta configuraciÃ³n permite a los desarrolladores ejecutar y probar la aplicaciÃ³n en un entorno que se asemeje al de producciÃ³n, facilitando el desarrollo y la depuraciÃ³n de problemas antes de desplegar el cÃ³digo en entornos crÃ­ticos.

â“Â¿CÃ³mo se hace que un cambio de cÃ³digo pase de dev a producciÃ³n?

âœ…Los cambios de cÃ³digo se trasladan de desarrollo a producciÃ³n a travÃ©s de un pipeline de CI meticulosamente configurado. Durante este proceso, el cÃ³digo se convierte en contenedores Docker y se somete a pruebas automatizadas para verificar su funcionalidad. DespuÃ©s de pasar exitosamente las pruebas en entornos de staging o QA, el pipeline de CD despliega automÃ¡ticamente los contenedores probados en producciÃ³n. Este enfoque controlado y automatizado minimiza el riesgo de fallos en producciÃ³n y asegura que las nuevas versiones se desplieguen de manera segura y eficiente.

â“Â¿CÃ³mo alguien aÃ±ade un componente a una pila?

âœ…Incorporar un nuevo componente a la pila implica la creaciÃ³n de un Dockerfile para el servicio adicional y la actualizaciÃ³n del archivo Compose correspondiente. Estos ajustes se integran en el pipeline de CI/CD, donde el nuevo componente se construye, prueba y despliega automÃ¡ticamente en los entornos de desarrollo y producciÃ³n. Este proceso garantiza que la integraciÃ³n del nuevo componente sea uniforme y eficaz, manteniendo la estabilidad del sistema mientras se amplÃ­a la infraestructura segÃºn sea necesario.